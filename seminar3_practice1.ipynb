{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WnHV5hlDIpcL",
        "outputId": "d40007b6-d739-4726-8e54-0ebbd1410243"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 2/118 [00:00<00:06, 16.65it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|▎         | 4/118 [00:00<00:06, 16.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▌         | 6/118 [00:00<00:06, 16.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  7%|▋         | 8/118 [00:00<00:06, 16.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  8%|▊         | 10/118 [00:00<00:06, 16.81it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 12/118 [00:00<00:06, 16.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 12%|█▏        | 14/118 [00:00<00:06, 16.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 14%|█▎        | 16/118 [00:00<00:06, 16.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 15%|█▌        | 18/118 [00:01<00:05, 16.92it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 17%|█▋        | 20/118 [00:01<00:05, 16.93it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 19%|█▊        | 22/118 [00:01<00:05, 16.93it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 24/118 [00:01<00:05, 16.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 22%|██▏       | 26/118 [00:01<00:05, 16.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 24%|██▎       | 28/118 [00:01<00:05, 16.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 25%|██▌       | 30/118 [00:01<00:05, 16.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 29%|██▉       | 34/118 [00:02<00:05, 14.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 32%|███▏      | 38/118 [00:02<00:05, 15.53it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 36%|███▌      | 42/118 [00:02<00:04, 16.26it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 39%|███▉      | 46/118 [00:02<00:04, 16.61it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 42%|████▏     | 50/118 [00:03<00:04, 16.81it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 46%|████▌     | 54/118 [00:03<00:03, 16.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 49%|████▉     | 58/118 [00:03<00:03, 16.82it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 53%|█████▎    | 62/118 [00:03<00:03, 16.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 56%|█████▌    | 66/118 [00:04<00:03, 16.84it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 59%|█████▉    | 70/118 [00:04<00:02, 16.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 63%|██████▎   | 74/118 [00:04<00:02, 16.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 66%|██████▌   | 78/118 [00:04<00:02, 16.94it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n",
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 67%|██████▋   | 79/118 [00:04<00:02, 16.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([512, 256, 4, 4])\n",
            "torch.Size([512, 128, 8, 8]) torch.Size([512, 128, 8, 8])\n",
            "torch.Size([512, 64, 16, 16]) torch.Size([512, 64, 16, 16])\n",
            "torch.Size([512, 32, 32, 32]) torch.Size([512, 32, 32, 32])\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[17], line 180\u001b[0m\n\u001b[1;32m    178\u001b[0m loss_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    179\u001b[0m cnt \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, lbls \u001b[38;5;129;01min\u001b[39;00m tqdm(dataloader):\n\u001b[1;32m    181\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    182\u001b[0m     x \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n",
            "File \u001b[0;32m~/algo_seminar/.venv/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
            "File \u001b[0;32m~/algo_seminar/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
            "File \u001b[0;32m~/algo_seminar/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[0;32m~/algo_seminar/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m~/algo_seminar/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m~/algo_seminar/.venv/lib/python3.10/site-packages/torchvision/datasets/mnist.py:146\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    143\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
            "File \u001b[0;32m~/algo_seminar/.venv/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
            "File \u001b[0;32m~/algo_seminar/.venv/lib/python3.10/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/algo_seminar/.venv/lib/python3.10/site-packages/torchvision/transforms/functional.py:168\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# handle PIL Image\u001b[39;00m\n\u001b[1;32m    167\u001b[0m mode_to_nptype \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint32, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI;16\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mbyteorder \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlittle\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI;16B\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint16, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mfloat32}\n\u001b[0;32m--> 168\u001b[0m img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_to_nptype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    171\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n",
            "File \u001b[0;32m~/algo_seminar/.venv/lib/python3.10/site-packages/PIL/Image.py:756\u001b[0m, in \u001b[0;36mImage.__array_interface__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    754\u001b[0m     new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtobytes(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 756\u001b[0m     new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    757\u001b[0m new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m], new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtypestr\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _conv_type_shape(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new\n",
            "File \u001b[0;32m~/algo_seminar/.venv/lib/python3.10/site-packages/PIL/Image.py:811\u001b[0m, in \u001b[0;36mImage.tobytes\u001b[0;34m(self, encoder_name, *args)\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;66;03m# unpack data\u001b[39;00m\n\u001b[0;32m--> 811\u001b[0m e \u001b[38;5;241m=\u001b[39m \u001b[43m_getencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m e\u001b[38;5;241m.\u001b[39msetimage(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mim)\n\u001b[1;32m    814\u001b[0m bufsize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m65536\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m)  \u001b[38;5;66;03m# see RawEncode.c\u001b[39;00m\n",
            "File \u001b[0;32m~/algo_seminar/.venv/lib/python3.10/site-packages/PIL/Image.py:463\u001b[0m, in \u001b[0;36m_getencoder\u001b[0;34m(mode, encoder_name, args, extra)\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoder(mode, \u001b[38;5;241m*\u001b[39margs \u001b[38;5;241m+\u001b[39m extra)\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;66;03m# get encoder\u001b[39;00m\n\u001b[0;32m--> 463\u001b[0m     encoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mencoder_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_encoder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    465\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mencoder_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not available\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from diffusers import DDPMScheduler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Hyperparameters\n",
        "img_size = 32\n",
        "batch_size = 512\n",
        "max_steps = 1000\n",
        "lr = 1e-3\n",
        "epochs = 1000\n",
        "\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=1, features=None):\n",
        "        super(CustomModel, self).__init__()\n",
        "        if features is None:\n",
        "            features = [32, 64, 128]\n",
        "\n",
        "        # Encoder: Downsampling layers with skip connections\n",
        "        self.encoders = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(in_channels if i == 0 else features[i - 1], features[i], kernel_size=3, padding=1),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(features[i], features[i], kernel_size=3, padding=1),\n",
        "                nn.ReLU(inplace=True),\n",
        "            )\n",
        "            for i in range(len(features))\n",
        "        ])\n",
        "\n",
        "        # Pooling layers\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            nn.Conv2d(features[-1], features[-1] * 2, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(features[-1] * 2, features[-1] * 2, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        # Decoder: Upsampling layers\n",
        "        self.decoders = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(features[i], features[i], kernel_size=3, padding=1),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(features[i], features[i], kernel_size=3, padding=1),\n",
        "                nn.ReLU(inplace=True),\n",
        "            )\n",
        "            for i in reversed(range(len(features)))\n",
        "        ])\n",
        "\n",
        "        # Upsample layers\n",
        "        self.upsamples = nn.ModuleList([\n",
        "            nn.ConvTranspose2d(features[i] * 2 if i == len(features) - 1 else features[i + 1], features[i], kernel_size=2, stride=2)\n",
        "            for i in reversed(range(len(features)))\n",
        "        ])\n",
        "\n",
        "        # Final convolution\n",
        "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip_connections = []\n",
        "\n",
        "        # Encoder\n",
        "        for encoder in self.encoders:\n",
        "            x = encoder(x)\n",
        "            skip_connections.append(x)\n",
        "            x = self.pool(x)\n",
        "            \n",
        "        # Bottleneck\n",
        "        x = self.bottleneck(x)\n",
        "        # Decoder\n",
        "        skip_connections = skip_connections[::-1]\n",
        "        for i, (upsample, decoder) in enumerate(zip(self.upsamples, self.decoders)):\n",
        "            x = upsample(x)  # Upsample\n",
        "            x = x + skip_connections[i]  # Add skip connection\n",
        "            x = decoder(x)  # Apply decoder layers\n",
        "\n",
        "        return self.final_conv(x)\n",
        "\n",
        "    \n",
        "def prepare_dataset(batch_size):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    # Load FashionMNIST Dataset\n",
        "    dataset = datasets.FashionMNIST(root=\"./data\", download=True, transform=transform)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    return dataloader\n",
        "\n",
        "def show_images(images, rows=2, cols=10):\n",
        "    _, axes = plt.subplots(rows, cols, figsize=(cols, rows))\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        ax.imshow(images[i].cpu().numpy().squeeze(), cmap='gray')\n",
        "        ax.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def diffusion_process_and_show_images(scheduler, model):\n",
        "    # Diffusion Process\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        samples = torch.randn((20, 1, img_size, img_size), device=device)\n",
        "        for _, t in tqdm(enumerate(scheduler.timesteps)):\n",
        "            samples = scheduler.step(model(samples), t, samples).prev_sample\n",
        "    show_images([sample[0] for sample in samples])        \n",
        "\n",
        "def save_checkpoint(epoch, model, optimizer, loss_avg):\n",
        "    checkpoint_path = f\"data/FashionMNIST/checkpoint_epoch_{epoch}.pth\"\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss_avg,\n",
        "    }, checkpoint_path)\n",
        "    print(f\"Checkpoint saved at {checkpoint_path}\")\n",
        "        \n",
        "def load_checkpoint(checkpoint_path, model, optimizer):\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    loss = checkpoint['loss']\n",
        "    print(f\"Checkpoint loaded: Epoch {epoch}, Loss: {loss}\")\n",
        "    return epoch, loss\n",
        "\n",
        "def plot_losses(losses):\n",
        "    plt.plot(losses)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.show()\n",
        "\n",
        "# Select device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "\n",
        "# Prepare dataset\n",
        "dataloader = prepare_dataset(batch_size)\n",
        "\n",
        "# Initialize model\n",
        "#model = UNet2DModel(\n",
        "#    sample_size=img_size,\n",
        "#    in_channels=1,\n",
        "#    out_channels=1,\n",
        "#    layers_per_block=2,\n",
        "#    block_out_channels=(32, 64, 128),\n",
        "#    down_block_types=(\"DownBlock2D\", \"DownBlock2D\", \"AttnDownBlock2D\"),\n",
        "#    up_block_types=(\"AttnUpBlock2D\", \"UpBlock2D\", \"UpBlock2D\"),\n",
        "#).to(device)\n",
        "# Model Initialization\n",
        "model = CustomModel(in_channels=1, out_channels=1).to(device)\n",
        "\n",
        "# Scheduler for Diffusion\n",
        "scheduler = DDPMScheduler(num_train_timesteps=max_steps)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "start_epoch = 0\n",
        "# Load Checkpoint path if needed\n",
        "# checkpoint_path = \"data/FashionMNIST/checkpoint_epoch_274.pth\"\n",
        "# if os.path.exists(checkpoint_path):\n",
        "#    start_epoch, _ = load_checkpoint(checkpoint_path, model, optimizer)\n",
        "\n",
        "# Training loop\n",
        "losses = []\n",
        "for epoch in range(start_epoch, epochs):\n",
        "    model.train()\n",
        "    loss_sum = 0.0\n",
        "    cnt = 0\n",
        "    for images, lbls in tqdm(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        x = images.to(device)\n",
        "\n",
        "        # Randam time step\n",
        "        t = torch.randint(0, max_steps, (len(x),), device=device)  \n",
        "\n",
        "        # Add noise to images\n",
        "        noise = torch.randn_like(x)\n",
        "        noisy_images = scheduler.add_noise(x, noise, t)            \n",
        "\n",
        "        # Predict noise\n",
        "        noise_pred = model(noisy_images)\n",
        "\n",
        "        # Back Propagation\n",
        "        loss = F.mse_loss(noise_pred, noise)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_sum += loss.item()\n",
        "        cnt += 1\n",
        "\n",
        "    loss_avg = loss_sum / cnt\n",
        "    losses.append(loss_avg)\n",
        "    print(f'Epoch {epoch} | Loss: {loss_avg}')\n",
        "\n",
        "    save_checkpoint(epoch, model, optimizer, loss_avg)\n",
        "    plot_losses(losses)\n",
        "\n",
        "    # Generate and visualize samples with last batch\n",
        "    diffusion_process_and_show_images(scheduler, model)\n",
        "\n",
        "# Save model\n",
        "model.save_pretrained(\"data/FashionMNIST/\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
