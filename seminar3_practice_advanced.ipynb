{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WnHV5hlDIpcL",
    "outputId": "d40007b6-d739-4726-8e54-0ebbd1410243"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [00:25<00:00,  4.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss: 0.06574918910608453\n",
      "Checkpoint saved at data/FashionMNIST/checkpoint_epoch_0.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[96, -1, 8, 16]' is invalid for input of size 163840",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 164\u001b[0m\n\u001b[1;32m    161\u001b[0m     plot_losses(losses)\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# Generate and visualize samples with last batch\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m     \u001b[43mdiffusion_process_and_show_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# Save model\u001b[39;00m\n\u001b[1;32m    167\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/FashionMNIST/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 59\u001b[0m, in \u001b[0;36mdiffusion_process_and_show_images\u001b[0;34m(scheduler, model, text_embeddings)\u001b[0m\n\u001b[1;32m     57\u001b[0m     samples \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn((\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m1\u001b[39m, img_size, img_size), device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, t \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(scheduler\u001b[38;5;241m.\u001b[39mtimesteps)):\n\u001b[0;32m---> 59\u001b[0m         samples \u001b[38;5;241m=\u001b[39m scheduler\u001b[38;5;241m.\u001b[39mstep(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_embeddings\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample, t, samples)\u001b[38;5;241m.\u001b[39mprev_sample\n\u001b[1;32m     60\u001b[0m show_images([sample[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m samples])\n",
      "File \u001b[0;32m~/algo_seminar/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/algo_seminar/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/algo_seminar/.venv/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_condition.py:1246\u001b[0m, in \u001b[0;36mUNet2DConditionModel.forward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, down_intrablock_additional_residuals, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[1;32m   1244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmid_block \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1245\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmid_block, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_cross_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmid_block\u001b[38;5;241m.\u001b[39mhas_cross_attention:\n\u001b[0;32m-> 1246\u001b[0m         sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmid_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m            \u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m            \u001b[49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1254\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1255\u001b[0m         sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmid_block(sample, emb)\n",
      "File \u001b[0;32m~/algo_seminar/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/algo_seminar/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/algo_seminar/.venv/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_blocks.py:889\u001b[0m, in \u001b[0;36mUNetMidBlock2DCrossAttn.forward\u001b[0;34m(self, hidden_states, temb, encoder_hidden_states, attention_mask, cross_attention_kwargs, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    882\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    883\u001b[0m             create_custom_forward(resnet),\n\u001b[1;32m    884\u001b[0m             hidden_states,\n\u001b[1;32m    885\u001b[0m             temb,\n\u001b[1;32m    886\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mckpt_kwargs,\n\u001b[1;32m    887\u001b[0m         )\n\u001b[1;32m    888\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    897\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m resnet(hidden_states, temb)\n\u001b[1;32m    899\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/algo_seminar/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/algo_seminar/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/algo_seminar/.venv/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:442\u001b[0m, in \u001b[0;36mTransformer2DModel.forward\u001b[0;34m(self, hidden_states, encoder_hidden_states, timestep, added_cond_kwargs, class_labels, cross_attention_kwargs, attention_mask, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[1;32m    430\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    431\u001b[0m             create_custom_forward(block),\n\u001b[1;32m    432\u001b[0m             hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mckpt_kwargs,\n\u001b[1;32m    440\u001b[0m         )\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 442\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclass_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# 3. Output\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_input_continuous:\n",
      "File \u001b[0;32m~/algo_seminar/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/algo_seminar/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/algo_seminar/.venv/lib/python3.10/site-packages/diffusers/models/attention.py:545\u001b[0m, in \u001b[0;36mBasicTransformerBlock.forward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, timestep, cross_attention_kwargs, class_labels, added_cond_kwargs)\u001b[0m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mada_norm_single\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    543\u001b[0m         norm_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed(norm_hidden_states)\n\u001b[0;32m--> 545\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m attn_output \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    553\u001b[0m \u001b[38;5;66;03m# 4. Feed-forward\u001b[39;00m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;66;03m# i2vgen doesn't have this norm ü§∑‚Äç‚ôÇÔ∏è\u001b[39;00m\n",
      "File \u001b[0;32m~/algo_seminar/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/algo_seminar/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/algo_seminar/.venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:495\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, hidden_states, encoder_hidden_states, attention_mask, **cross_attention_kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    491\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcross_attention_kwargs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_kwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m are not expected by \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and will be ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    492\u001b[0m     )\n\u001b[1;32m    493\u001b[0m cross_attention_kwargs \u001b[38;5;241m=\u001b[39m {k: w \u001b[38;5;28;01mfor\u001b[39;00m k, w \u001b[38;5;129;01min\u001b[39;00m cross_attention_kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m attn_parameters}\n\u001b[0;32m--> 495\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/algo_seminar/.venv/lib/python3.10/site-packages/diffusers/models/attention_processor.py:2371\u001b[0m, in \u001b[0;36mAttnProcessor2_0.__call__\u001b[0;34m(self, attn, hidden_states, encoder_hidden_states, attention_mask, temb, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2368\u001b[0m inner_dim \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   2369\u001b[0m head_dim \u001b[38;5;241m=\u001b[39m inner_dim \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m attn\u001b[38;5;241m.\u001b[39mheads\n\u001b[0;32m-> 2371\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m   2373\u001b[0m key \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, attn\u001b[38;5;241m.\u001b[39mheads, head_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m   2374\u001b[0m value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, attn\u001b[38;5;241m.\u001b[39mheads, head_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[96, -1, 8, 16]' is invalid for input of size 163840"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from diffusers import UNet2DConditionModel, DDPMScheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hyperparameters\n",
    "img_size = 32\n",
    "batch_size = 512\n",
    "max_steps = 1000\n",
    "lr = 1e-3\n",
    "epochs = 1000\n",
    "\n",
    "def prepare_dataset_with_labels(batch_size):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    # Load FashionMNIST Dataset\n",
    "    dataset = datasets.FashionMNIST(root=\"./data\", download=True, transform=transform)\n",
    "\n",
    "    # „ÇØ„É©„Çπ„É©„Éô„É´„ÅÆ„Éû„ÉÉ„Éî„É≥„Ç∞\n",
    "    labels = {\n",
    "        0: \"T-shirt\",\n",
    "        1: \"Trouser\",\n",
    "        2: \"Pullover\",\n",
    "        3: \"Dress\",\n",
    "        4: \"Coat\",\n",
    "        5: \"Sandal\",\n",
    "        6: \"Shirt\",\n",
    "        7: \"Sneaker\",\n",
    "        8: \"Bag\",\n",
    "        9: \"Ankle boot\"\n",
    "    }\n",
    "\n",
    "    # DataLoader„ÅÆ‰ΩúÊàê\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader, labels\n",
    "\n",
    "def show_images(images, rows=2, cols=10):\n",
    "    _, axes = plt.subplots(rows, cols, figsize=(cols, rows))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(images[i].cpu().numpy().squeeze(), cmap='gray')\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def diffusion_process_and_show_images(scheduler, model, text_embeddings):\n",
    "    # Diffusion Process\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        samples = torch.randn((20, 1, img_size, img_size), device=device)\n",
    "        for _, t in tqdm(enumerate(scheduler.timesteps)):\n",
    "            samples = scheduler.step(model(samples,t,encoder_hidden_states=text_embeddings[:20]).sample, t, samples).prev_sample\n",
    "    show_images([sample[0] for sample in samples])\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, loss_avg):\n",
    "    checkpoint_path = f\"data/FashionMNIST/checkpoint_epoch_{epoch}.pth\"\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss_avg,\n",
    "    }, checkpoint_path)\n",
    "    print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "        \n",
    "def load_checkpoint(checkpoint_path, model, optimizer):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    print(f\"Checkpoint loaded: Epoch {epoch}, Loss: {loss}\")\n",
    "    return epoch, loss\n",
    "\n",
    "def plot_losses(losses):\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "\n",
    "# Select device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "# Prepare dataset and labels\n",
    "dataloader, labels = prepare_dataset_with_labels(batch_size)  # Ensure labels are loaded\n",
    "\n",
    "# Initialize model, CLIP, and scheduler\n",
    "model = UNet2DConditionModel(\n",
    "    sample_size=img_size,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(32, 64, 128),\n",
    "    down_block_types=(\"DownBlock2D\", \"DownBlock2D\", \"AttnDownBlock2D\"),\n",
    "    up_block_types=(\"AttnUpBlock2D\", \"UpBlock2D\", \"UpBlock2D\"),\n",
    "    cross_attention_dim=512  # Match CLIP's embedding size\n",
    ").to(device)\n",
    "\n",
    "# Scheduler for Diffusion\n",
    "scheduler = DDPMScheduler(num_train_timesteps=max_steps)\n",
    "\n",
    "# CLIP components\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "start_epoch = 0\n",
    "# Load Checkpoint path if needed\n",
    "# checkpoint_path = \"data/FashionMNIST/checkpoint_epoch_274.pth\"\n",
    "# if os.path.exists(checkpoint_path):\n",
    "#    start_epoch, _ = load_checkpoint(checkpoint_path, model, optimizer)\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    model.train()\n",
    "    loss_sum = 0.0\n",
    "    cnt = 0\n",
    "    for images, lbls in tqdm(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        x = images.to(device)\n",
    "\n",
    "        # Randam time step\n",
    "        t = torch.randint(0, max_steps, (len(x),), device=device)  \n",
    "\n",
    "        # Add noise to images\n",
    "        noise = torch.randn_like(x)\n",
    "        noisy_images = scheduler.add_noise(x, noise, t)            \n",
    "\n",
    "        # „É©„Éô„É´„ÉÜ„É≥„ÇΩ„É´„Åã„Çâ„Éó„É≠„É≥„Éó„Éà„ÇíÁîüÊàê\n",
    "        prompts = [labels[lbl.item()] for lbl in lbls]\n",
    "\n",
    "        # „ÉÜ„Ç≠„Çπ„ÉàÂüã„ÇÅËæº„Åø„ÅÆÂèñÂæó\n",
    "        tokens = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        text_embeddings = text_encoder(**tokens).last_hidden_state\n",
    "\n",
    "        # Predict noise using cross-attention\n",
    "        noise_pred = model(noisy_images, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "        # Back Propagation\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.item()\n",
    "        cnt += 1\n",
    "\n",
    "    loss_avg = loss_sum / cnt\n",
    "    losses.append(loss_avg)\n",
    "    print(f'Epoch {epoch} | Loss: {loss_avg}')\n",
    "\n",
    "    save_checkpoint(epoch, model, optimizer, loss_avg)\n",
    "    plot_losses(losses)\n",
    "\n",
    "    # Generate and visualize samples with last batch\n",
    "    diffusion_process_and_show_images(scheduler, model, text_embeddings)\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained(\"data/FashionMNIST/\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
