{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WnHV5hlDIpcL",
        "outputId": "d40007b6-d739-4726-8e54-0ebbd1410243"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from diffusers import UNet2DModel, DDPMScheduler\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Hyperparameters\n",
        "img_size = 32\n",
        "batch_size = 512\n",
        "max_steps = 1000\n",
        "lr = 1e-3\n",
        "epochs = 1000\n",
        "\n",
        "def prepare_dataset(batch_size):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    # Load FashionMNIST Dataset\n",
        "    dataset = datasets.FashionMNIST(root=\"./data\", download=True, transform=transform)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    return dataloader\n",
        "\n",
        "def show_images(images, rows=2, cols=10):\n",
        "    _, axes = plt.subplots(rows, cols, figsize=(cols, rows))\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        ax.imshow(images[i].cpu().numpy().squeeze(), cmap='gray')\n",
        "        ax.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def diffusion_process_and_show_images(scheduler, model):\n",
        "    # Diffusion Process\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        samples = torch.randn((20, 1, img_size, img_size), device=device)\n",
        "        for _, t in tqdm(enumerate(scheduler.timesteps)):\n",
        "            samples = scheduler.step(model(samples, t).sample, t, samples).prev_sample\n",
        "    show_images([sample[0] for sample in samples])        \n",
        "\n",
        "def save_checkpoint(epoch, model, optimizer, loss_avg):\n",
        "    checkpoint_path = f\"data/FashionMNIST/checkpoint_epoch_{epoch}.pth\"\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss_avg,\n",
        "    }, checkpoint_path)\n",
        "    print(f\"Checkpoint saved at {checkpoint_path}\")\n",
        "        \n",
        "def load_checkpoint(checkpoint_path, model, optimizer):\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    loss = checkpoint['loss']\n",
        "    print(f\"Checkpoint loaded: Epoch {epoch}, Loss: {loss}\")\n",
        "    return epoch, loss\n",
        "\n",
        "def plot_losses(losses):\n",
        "    plt.plot(losses)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.show()\n",
        "\n",
        "# Select device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "\n",
        "# Prepare dataset\n",
        "dataloader = prepare_dataset(batch_size)\n",
        "\n",
        "# Initialize model\n",
        "model = UNet2DModel(\n",
        "    sample_size=img_size,\n",
        "    in_channels=1,\n",
        "    out_channels=1,\n",
        "    layers_per_block=2,\n",
        "    block_out_channels=(32, 64, 128),\n",
        "    down_block_types=(\"DownBlock2D\", \"DownBlock2D\", \"AttnDownBlock2D\"),\n",
        "    up_block_types=(\"AttnUpBlock2D\", \"UpBlock2D\", \"UpBlock2D\"),\n",
        ").to(device)\n",
        "\n",
        "# Scheduler for Diffusion\n",
        "scheduler = DDPMScheduler(num_train_timesteps=max_steps)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "start_epoch = 0\n",
        "# Load Checkpoint path if needed\n",
        "# checkpoint_path = \"data/FashionMNIST/checkpoint_epoch_274.pth\"\n",
        "# if os.path.exists(checkpoint_path):\n",
        "#    start_epoch, _ = load_checkpoint(checkpoint_path, model, optimizer)\n",
        "\n",
        "# Training loop\n",
        "losses = []\n",
        "for epoch in range(start_epoch, epochs):\n",
        "    model.train()\n",
        "    loss_sum = 0.0\n",
        "    cnt = 0\n",
        "    for images, lbls in tqdm(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        x = images.to(device)\n",
        "\n",
        "        # Randam time step\n",
        "        t = torch.randint(0, max_steps, (len(x),), device=device)  \n",
        "\n",
        "        # Add noise to images\n",
        "        noise = torch.randn_like(x)\n",
        "        noisy_images = scheduler.add_noise(x, noise, t)            \n",
        "\n",
        "        # Predict noise\n",
        "        noise_pred = model(noisy_images, t).sample\n",
        "\n",
        "        # Back Propagation\n",
        "        loss = F.mse_loss(noise_pred, noise)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_sum += loss.item()\n",
        "        cnt += 1\n",
        "\n",
        "    loss_avg = loss_sum / cnt\n",
        "    losses.append(loss_avg)\n",
        "    print(f'Epoch {epoch} | Loss: {loss_avg}')\n",
        "\n",
        "    save_checkpoint(epoch, model, optimizer, loss_avg)\n",
        "    plot_losses(losses)\n",
        "\n",
        "    # Generate and visualize samples with last batch\n",
        "    diffusion_process_and_show_images(scheduler, model)\n",
        "\n",
        "# Save model\n",
        "model.save_pretrained(\"data/FashionMNIST/\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
